{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dépendance__ : Ce Workshop doit être réalisé après celui sur le _data wrangling_.\n",
    "\n",
    "Nous allons essayer, dans le cadre de ce workshop, d'entraîner, de configurer et d'utiliser des algorithmes _ML_ de régression. Nous tâcherons systématiquement d'en discuter les performances en nous appuyant sur des indicateurs pertinents.\n",
    "\n",
    "On rappelle que le _dataset_ présente des données immobilières californiennes. Il compte des variables telles que la population, le salaire médian, le prix moyen d'un logement, _etc_. Et ce pour chaque _block group_ (le _block group_ est la la plus petite division administrative aux Etats-Unis - entre 500 et 5000 personnes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappel des objectifs\n",
    "\n",
    "On cherche à mettre au point un modèle de prédiction du prix médian d'un logement en fonction des autres informations. C'est clairement un problème [_supervisé_](https://fr.wikipedia.org/wiki/Apprentissage_supervis%C3%A9) de [_régression multivarié_](https://fr.wikipedia.org/wiki/R%C3%A9gression_lin%C3%A9aire_multiple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesure de performance\n",
    "\n",
    "On s'intéressera à terme à la minimisation de la [_root mean squared error (RMSE)_](https://en.wikipedia.org/wiki/Root-mean-square_deviation) et/ou de à la [_mean absolute error (MAE)_](https://en.wikipedia.org/wiki/Mean_absolute_error) de notre modèle :\n",
    "\n",
    "$$\n",
    "\\newcommand\\x{\\mathbf{x}}\n",
    "\\newcommand\\X{\\mathbf{X}}\n",
    "\\newcommand\\y{\\mathbf{y}}\n",
    "\\newcommand\\Y{\\mathbf{Y}}\n",
    "\\newcommand\\RMSE{\\mbox{RMSE}}\n",
    "\\newcommand\\MAE{\\mbox{MAE}}\n",
    "$$\n",
    "$$\n",
    "\\RMSE(\\X,h)=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}{(h(\\x^{(i)})-y^{(i)})^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\MAE(\\X,h)=\\frac{\\sum_{i=1}^{m}{\\lvert h(\\x^{(i)})-y^{(i)})\\rvert}}{m}\n",
    "$$\n",
    "\n",
    "$h$ étant la fonction de prédiction du modèle. Nous y revidenront plus bas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compatibilité python 2 et python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# stabilité du notebook d'une exécution à l'autre\n",
    "np.random.seed(42)\n",
    "\n",
    "# jolies figures directement dans le notebook\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# où sauver les figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"workflowDS\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID) # le dossier doit exister\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# ignorer les warnings inutiles (voir SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction de l'environnement de fin de la première partie du _Workshop_\n",
    "\n",
    "On va reconstituer le _kernel_ `python` de la première partie du _Workshop_. Vous pouvez executer le code sans vous soucier de son contenu jusqu'à la partie _Choisir et Entraîner un modèle_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import et chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://github.com/ph10r/eiSpeInfoDS/raw/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "fetch_housing_data()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"]/1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True) # replace where false\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline de préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "housing_num = housing.select_dtypes(include=[np.number]) \n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choisir et Entraîner un modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le travail effectué lors du précédent _workshop_ a abouti à plusieurs résultats :\n",
    "\n",
    "- Nous avons réservé un jeu de données de test stratifié `strat_test_set`,\n",
    "- nous en avons extrait les variables d'entrée `housing` et la colonne cible `housing_labels`,\n",
    "- nous avons créé un _pipeline_ `full_pipeline` de préparation de la donné d'entraînement.\n",
    "- nous avons utilisé ce _pipeline_ pour façonner un jeu de données `housing_prepared`.\n",
    "\n",
    "Il est naturel de vouloir maintenant entrer dans le vif du sujet : le _Machine Learning_.\n",
    "\n",
    "Entraînons un modèle de [_régression linéaire_](https://fr.wikipedia.org/wiki/R%C3%A9gression_lin%C3%A9aire) avec nos données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Premier modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est tout.\n",
    "Notre modèle est prêt à faire des prédiction.\n",
    "> **Remarque**: Nous allons lui présenter des données du jeux d'entraînement en prenant soin d'y appliquer\n",
    "le _pipeline_ de transformation, comme nous le ferions avec des données nouvelles. Utiliser directement des données de `housing_prepared` menerait au même résultat mais la démarche ne serait pas transposable à d'autre données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique le full_pipeline sur quelques instances :\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "# Et on effectue la prédiction :\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut comparer ces prédictions aux vraies valeurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels:\", list(some_labels)) # vraies valeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluer / valider un modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos résultats ne semblent pas abérrants _au premier coup d'oeil_, mais il est naturel de chercher à quantifier notre erreur. C'est là qu'interviennent la [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) et la [MAE](https://en.wikipedia.org/wiki/Mean_absolute_error).\n",
    "\n",
    "#### Mesures de performance\n",
    "\n",
    "Pour rappel\n",
    "$\n",
    "\\RMSE(\\X,h)=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}{(h(\\x^{(i)})-y^{(i)})^2}}\n",
    "$\n",
    "et\n",
    "$\n",
    "\\MAE(\\X,h)=\\frac{1}{m}\\sum_{i=1}^{m}{\\lvert h(\\x^{(i)})-y^{(i)})\\rvert}\n",
    "$\n",
    "\n",
    "Il est bon de s'arrêter un instant pour comprendre ces formules. Toutes deux se ressemblent :\n",
    "\n",
    "- on prend pour chaque valeur prédite la distance algébrique à la vrai valeur,\n",
    "- on la transforme en une valeur positive pour obtenir une distance: soit en élevant au carré, soit _via_ la valeur absolue,\n",
    "- on fait la moyenne de toutes ces distances,\n",
    "- on passe à la racine carré dans le premier cas par souci d'échelle et de lisibilité.\n",
    "\n",
    "L'intuition peut donc appréhender ces mesures comme étant des quantifications de l'erreur moyenne d'un modèle prédictif. La première pénalisera fortement les erreurs importantes et sera indulgente envers les erreurs faibles.\n",
    "\n",
    "`scikit-learn` facilite le calcul de ces métrique, par exemple grâce à la fonction [`mean_squared_error`](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En exercice, on peut explorer [une partie de la documentation](https://scikit-learn.org/stable/modules/model_evaluation.html) de `scikit-learn` pour calculer la _MAE_ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE A COMPLETER\n",
    "\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce n'est pas un résultat satisfaisant, en effet, on voit, en explorant les données, que la majorité des districts ont une `median_housing_value` comprise entre 120 et 265 mille dollars, une erreur typique de 68 mille dollars peut être acceptée par aucun client.\n",
    "\n",
    "> **Remarque** : il est encore trop tôt pour utiliser le jeu de test. En effet, nous sommes encore dans le processus d'élaboration d'un modèle performant. S'il on avait recours trop souvent au jeu de test durant cette phase, on courrait le risque d'obtenir un modèle _spécialisé_ pour ce je de test-là. Il serait sur-évalué et ne généraliserait pas _en production_ avec des scores similaires.\n",
    "\n",
    "> On garde à l'esprit : pas de _data snooping_ !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est en situation de _sous-apprentissage_ (ou _under-fitting_). Cela signifie :\n",
    "\n",
    "- soit que les données livrées à l'algorithmes d'apprentissage ne contiennent pas l'information permettant l'apprentissage,\n",
    "- soit que le modèle en lui-même n'est pas assez puissant pour capter toute l'information du jeu d'entraînement.\n",
    "\n",
    "Pour progresser, nous pouvons considérer plusieurs approches :\n",
    "\n",
    "1. soit relâcher les contraintes exercées sur les paramètres de notre modèle,\n",
    "1. soit modifier nos données d'entrée (par exemple : en ajoutant des variables, en passant les données dont l'histogramme est _tail-heavy_ au logarithme, _etc_.),\n",
    "1. soit sélectionner un modèle plus puissant,\n",
    "\n",
    "La première solution n'est pas envisageable ici car le modèle utilisé n'est pas hyper-paramétré. La deuxième n'est pas totalement exclue, mais il est trop tôt pour remettre en question notre (bon) travail du _Workshop_ précédent.\n",
    "\n",
    "\n",
    "#### Un modèle plus puissant\n",
    "\n",
    "Nous allons donc essayer un modèle plus puissant : un [_arbre de décision_](https://fr.wikipedia.org/wiki/Arbre_de_d%C3%A9cision_(apprentissage)) implémenté dans `scikit-learn` par [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html). A vous de créer est d'entraîner ce modèle. Pensez à initialiser la graine aléatoire du modèle (à $42$, bien évidemment) _via_ le paramètre `random_state` de son constructeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE A COMPLETER\n",
    "\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle est entraîné : on peut l'évaluer (toujours sur le traîning set, bien évidemment) et calculer l'erreur-type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avons-nous bien lu !? Pas d'erreur ? Le modèle parfait ?\n",
    "\n",
    "Il est bien plus probable que nous soyons en situation de [_sur-apprentissage_](https://fr.wikipedia.org/wiki/Surapprentissage) (ou _over-fitting_), le modèle, trop puissant, a pu se contenter d'apprendre _par coeur_ toutes nos données d'entraînement. Il est peu vraissemblale que cette stratégie _généralise_ sur des données nouvelles.\n",
    "\n",
    "On pourrait prouver cette théorie avec le jeu de test mais l'on s'est interdit de l'utiliser avant d'avoir la conviction que notre modèle était performant... Alors que faire ?\n",
    "\n",
    "Nous allons utiliser une partie du jeu d'entraînement pour entraîner à proprement dit, et une autre pour valider les modèles : c'est ce que l'on appelle la [_validation croisée_](https://fr.wikipedia.org/wiki/Validation_crois%C3%A9e) (ou _cross-validation_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation croisée\n",
    "\n",
    "On pourrait utiliser à nouveau `train_test_split` mais `scikit-learn` propose une [fonctionalité](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) de $k$-_fold cross validation_. Ce qui consiste à :\n",
    "\n",
    "- diviser aléatoirement le jeu d'entraînement en $k$ sous-jeux,\n",
    "- pour chacun des sous-jeux, tour à tour désignés _sous-jeu de validation_:\n",
    "    * entraîner le modèle sur les $k-1$ autres sous-jeux\n",
    "    * évaluer le modèle sur le sous-jeu de validation\n",
    "\n",
    "On obtient un _array_ de $k$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remarque** : `scikit-learn` attend non pas une mesure d'erreur pour la cross-validation mais une mesure de performance (le plus étant le mieux). Il est naturel d'utiliser l'opposé de la _MSE_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà qui est moins impressionant que le score parfait de tout à l'heure.\n",
    "\n",
    "On constate que l'erreur-type est de 71 mille dollars, soit supérieure à celle du modèle linéaire. L'_overfitting_ que nous suspections est avéré : ce modèle généralise mal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remarque** : La _cross-validation_ nous permet d'estimer la précision d'une estimation _via_ l'écart type. C'est une mesure précieuse mais chère : elle nécessite un nombre conséquent d'entraînement du modèle, ce qui n'est pas possible dans tous les contextes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par acquis de confiance, on peut calculer le même score pour le modèle linéaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE A COMPLETER\n",
    "                             #CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons la confirmation : notre modèle _sur-apprend_ tant et si mal qu'il est moins performant que le modèle linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Un troisième modèle\n",
    "\n",
    "Nous allons considérer la régréssion par _forêts aléatoires_ avec [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remarque** : Une forêt aléatoire est un ensemble d'arbres de régression entraînés sur des sous-ensembles aléatoires de données d'entraînement. Cette stratégie de composition d'algorithmes de _ML_ est appelée _Ensemble Learning_, d'où le nom du module contenant `RandomForestRegression`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vous d'entraîner un `RandomForestRegressor` avec les paramètres `random_state` fixé à $42$, et éventuellement `n_estimators` fixé à 10 (valeur par défaut amenée à changer à l'avenir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient un bien meilleur résultat : c'est prometteur. Cela dit, la différence entre le score sur le _training set_ et le score de _cross validation_ nous prouve qu'il y a toujours un _overfitting_ important.\n",
    "Il conviendrait dans un projet réel de :\n",
    "- simplifier le modèle,\n",
    "- le contraindre,\n",
    "- d'obtenir un jeu de données plus volumineux,\n",
    "- essayer un nombre important de modèle (_SVM_, _Réseaux neuronaux_, _etc_.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En exercice, on va tester la [_Support Vector Regression_](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) avec un _kernel_ linéaire et commenter le résultat :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramétrage du modèle (_fine tuning_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons maintenant déterminé parmi les modèles envisagés quelques bon candidats. Il s'agit maintenant d'optimiser leur paramétrage.\n",
    "La première idée serait de tester _a la mano_ diverses valeurs de chaque hyper-paramètre du modèle mais cette approche s'avèrerait fastidieuse, surtout si leur nombre était important.\n",
    "\n",
    "Voyons comment procéder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "\n",
    "`scikit-learn` propose la fonction [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) pour automatiser ce travail. Il faut lui fournir les valeurs à tester pour chaque hyperparamètre. Toutes les combinaisons sont évaluées par _cross-validation_.\n",
    "\n",
    "> **Remarque** : Il n'est pas nécessaire **aujourd'hui** de se soucier de la signification des hyperparamètres. Ce n'est pas l'objet de ce _workshop_. Il sera cependant important à l'avenir de connaître les arcanes des algorithmes de _ML_ car se sont vos outils de travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # essaye 12 (3×4) combinaisons des hyperparametres\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # puis essaye 6 (2×3) combinaisons avec bootstrap à False (True étant la valeur par défaut)\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# 5 sous-jeux de cross-val, ça fait en tout (12+6)*5=90 tours d'entraînement \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le meilleurs _tuning_ est :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remarque** : le meilleur résultat étant obtenu pour les plus grandes valeurs testées, il semblerait pertinent de proposer des valeurs encore plus grandes au `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peur obtenir directement le meilleur modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remarque** : si `GridSearchCV` reçoit le paramètre `refit=True` (comportement par défaut), le modèle est ré-entraîné sur tout le _training set_ avec le meilleur paramétrage trouvé. Celà peut s'avérer très utile si notre modèle est entraîné et paramétré en production avec des mises à jour du jeu d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les scores de _cross-validations_ calculés sont évidemment accessibles. A vous d'afficher le score moyen pour chaque paramétrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = grid_search.cv_results_\n",
    "#CODE A COMPLETER\n",
    "    #CODE A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons atteint un _RMSE_ inférieur à $49700$, ce qui constitue un net progrès en comparaison du score antérieur qui était supérieur à $52000$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Search\n",
    "\n",
    "Une autre méthode, qui passe mieux à l'échelle d'un grand espace de recherche de paramètres, et la recherche _randomisée_. Elle est similaire dans son utilisation à _grid search_ mais fixe au hasard la valeur des paramètres dans un intervalle donné. On a le contrôle sur le nombre d'itérations et sur les intervalles de recherche. Essayons [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) avec 10 itérations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE A COMPLETER\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }\n",
    "\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "                                #CODE A COMPLETER\n",
    "#CODE A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons les scores obtenus et commentons-les :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "    #CODE A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodes d'ensemble\n",
    "\n",
    "Nous ne traiterons pas ce sujet en profondeur ici. Il est tout de même à savoir que la stratégie consistant à combiner des modèles performants peut aboutir à l'obtention d'un modèle encore plus performant. C'est ce que l'on a déjà constaté avec les _forêts aléatoires_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyser les meilleurs modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En inspectant certains modèles, on peut glâner de précieuses information. Par exemple, on peut lister l'importance relative de chaque _feature_ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = random_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas très lisible. Présentons ces données autrement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate qu'une seule parmi les catégories que l'on a _one-hot_-encodées semble avoir une quelconque importance. On pourrait sans doute, dans un but de performance, effectuer un _drop_ des autres sans trop de conséquence sur la qualité du résultat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation sur le _jeu de test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre travail ayant abouti a un résultat suffisamment performant, nous pouvons _enfin_ passer à l'évaluation du modèle sur le _test set_. On applique le _pipeline_ de transformation à ce jeu de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remarque** : c'est `transform()` et non `fit_transform()` que l'on utilise. Le _pipeline_ a été entraîné sur le jeu d'entraînement pour le calcul des médianes, les mises à l'échelles, _etc_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = random_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "#CODE A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut calculer notre _RMSE_ final :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "#CODE A COMPLETER\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient un score encore meilleur que celui envisagé. Ce n'est pas le cas général car il est fréquent que le _bricolage_ des hyper-paramètres finisse par se spécialiser à notre _training set_. Il faut l'accepter. L'idée de retoucher aux hyper-paramètres _a posteriori_ pour enjoliver le résultat ne produirait rien de généralisable à de nouvelles données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est maintenant temps de documenter votre code et de préparer une présentation de votre travail au client. Elle devra contenir :\n",
    "- ce que vous avez appris des données\n",
    "- ce que vous avez tester\n",
    "- ce qui a bien fonctionné ou pas\n",
    "- les hypothèses que vous avez faites\n",
    "- les limites de votre système\n",
    "- de belles illustrations de vos affirmations\n",
    "- des affirmations-chocs telles que \"Le revenu moyen est le prédicteur numéro 1 de la valeur des logements.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en prod, monitoring et maintenance\n",
    "\n",
    "Le _GO_ de notre client en poche, nous pouvons donner vie à notre modèle en production. Il faudra :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Brancher les sources de données de production à notre système\n",
    "* Ecrire des tests\n",
    "* Ecrire des métriques de _monitoring_ de la performance pour pouvoir la surveiller car les modèles pas régulièrement entraînés ont tendance à voir leurs performances s'écrouler .L'expertise humaine est la bienvenue pour analyser ces chutes de performance\n",
    "* Monitorer la qualité des données en amont\n",
    "* Automatiser le _re-fit_ du modèle et le _back-up_ des modèles précédents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline complet : préparation et prédiction\n",
    "\n",
    "Si l'on voulait optimiser les _hyper-paramètres_ de préparation (ex : stratégie de l'_imputer_) *et* d'apprentissage au sein d'un même `(Grid|Random)SearchCV`, il nous faudrait un _pipeline_ unique pour les deux tâches. On l'obtient de cette manière :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline_with_predictor = Pipeline([\n",
    "        #CODE A COMPLETER\n",
    "        #CODE A COMPLETER\n",
    "    ])\n",
    "\n",
    "full_pipeline_with_predictor.fit(housing, housing_labels)\n",
    "full_pipeline_with_predictor.predict(some_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde / chargement d'un modèle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut sauvegarder et recharger un modèle grâce à [`joblib`](https://scikit-learn.org/stable/modules/model_persistence.html). C'est utile lorsque l'entraînement de ce modèle a nécessité beaucoup de temps-machine. Ce n'est pas très utile dans notre cas car les modèles sont simples et le jeu de données est peu volumineux. Nous allons tout de même sauvegarder et charger un modèle pour l'exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(svm_reg,\"housing_svm_reg_linearKernel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_loaded_model = joblib.load(\"housing_svm_reg_linearKernel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = my_loaded_model.predict(housing_prepared)\n",
    "loaded_model_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "loaded_model_rmse = np.sqrt(loaded_model_mse)\n",
    "loaded_model_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
